{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Memoryblock test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "authorship_tag": "ABX9TyNmYZnY/YzXY33jffpYu8M0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RossM/machine-learning-colabs/blob/main/Memoryblock_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "IfXEFmRvYdmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU status"
      ],
      "metadata": {
        "id": "Qv1x14G-ZjeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "simple_nvidia_smi_display = False#@param {type:\"boolean\"}\n",
        "if simple_nvidia_smi_display:\n",
        "    #!nvidia-smi\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_output)\n",
        "else:\n",
        "    #!nvidia-smi -i 0 -e 0\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_output)\n",
        "    nvidiasmi_ecc_note = subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_ecc_note)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WIdM1Kn3YcPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Google Drive"
      ],
      "metadata": {
        "id": "DoExgck_ZZBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4TGsnF08aWc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the data"
      ],
      "metadata": {
        "id": "jakoyk_jZWH3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwvZkapDUhlJ"
      },
      "outputs": [],
      "source": [
        "from os.path import exists\n",
        "\n",
        "if exists('/content'):\n",
        "    # We're running on Google Colab\n",
        "    DATA_DIR = \"/content/danbooru2021/images\"\n",
        "else:\n",
        "    DATA_DIR = \"~/danbooru2021/images\"\n",
        "\n",
        "!mkdir -p $DATA_DIR\n",
        "!rsync --progress --recursive --size-only --verbose rsync://176.9.41.242:873/danbooru2021/512px/0000/ $DATA_DIR/512px/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "pq_PSpPyZT8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "\n",
        "PACKAGES=\"dominate wandb einops tqdm ipywidgets\"\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  PACKAGES += \" cloud-tpu-client==0.10 torch==1.11.0 torchvision https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl\"\n",
        "else:\n",
        "  PACKAGES += \" torch==1.12.0+cu116 torchvision==0.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\"\n",
        "!{sys.executable} -m pip install $PACKAGES"
      ],
      "metadata": {
        "id": "dILRNjf-axy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define our model"
      ],
      "metadata": {
        "id": "ZbbGfN5dZQ4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, functools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import einops\n",
        "from torch.nn import init\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "# Based on code from pytorch-pix2pix https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
        "\n",
        "def default(val, d):\n",
        "    if val is not None:\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "def init_weights(net, init_type='normal', init_gain=0.02):\n",
        "    \"\"\"Initialize network weights.\n",
        "\n",
        "    Parameters:\n",
        "        net (network)   -- network to be initialized\n",
        "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
        "    work better for some applications. Feel free to try yourself.\n",
        "    \"\"\"\n",
        "    def init_func(m):  # define the initialization function\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
        "            init.normal_(m.weight.data, 1.0, init_gain)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    print('initialize network with %s' % init_type)\n",
        "    net.apply(init_func)  # apply the initialization function <init_func>\n",
        "\n",
        "\n",
        "def init_net(net, init_type='normal', init_gain=0.02, devices=[\"cpu\"], dtype=torch.float):\n",
        "    \"\"\"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n",
        "    Parameters:\n",
        "        net (network)      -- the network to be initialized\n",
        "        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "        gain (float)       -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    Return an initialized network.\n",
        "    \"\"\"\n",
        "    net.to(dtype)\n",
        "    if str(devices[0]) != \"cpu\":\n",
        "        net.to(devices[0])\n",
        "        net = torch.nn.DataParallel(net, devices)  # multi-GPUs\n",
        "    init_weights(net, init_type, init_gain=init_gain)\n",
        "    return net\n",
        "\n",
        "def get_scheduler(optimizer, opt):\n",
        "    \"\"\"Return a learning rate scheduler\n",
        "\n",
        "    Parameters:\n",
        "        optimizer          -- the optimizer of the network\n",
        "        opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions．　\n",
        "                              opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\n",
        "\n",
        "    For 'linear', we keep the same learning rate for the first <opt.n_epochs> epochs\n",
        "    and linearly decay the rate to zero over the next <opt.n_epochs_decay> epochs.\n",
        "    For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.\n",
        "    See https://pytorch.org/docs/stable/optim.html for more details.\n",
        "    \"\"\"\n",
        "    def lambda_rule(epoch):\n",
        "        lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.n_epochs) / float(opt.n_epochs_decay + 1)\n",
        "        return lr_l\n",
        "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
        "    return scheduler\n",
        "\n",
        "##############################################################################\n",
        "# NN-Blocks\n",
        "##############################################################################\n",
        "\n",
        "class SequentialExt(nn.Sequential):\n",
        "    def forward(self, *args):\n",
        "        modules = list(self)\n",
        "        input = modules[0](*args)\n",
        "        for module in modules[1:]:\n",
        "            input = module(input)\n",
        "        return input\n",
        "\n",
        "def nameof(obj):\n",
        "    if hasattr(obj, '__name__'):\n",
        "        return obj.__name__\n",
        "    return str(obj)\n",
        "\n",
        "class Residual(nn.Sequential):\n",
        "    def __init__(self, *args, reduce=None):\n",
        "        super().__init__(*args)\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input\n",
        "        for module in self:\n",
        "            x = module(x)\n",
        "        if self.reduce != None:\n",
        "            return self.reduce(x, input)\n",
        "        else:\n",
        "            return x + input\n",
        "\n",
        "    def extra_repr(self):\n",
        "        if self.reduce != None:\n",
        "            return str(f\"reduce={nameof(self.reduce)}\")\n",
        "\n",
        "class Bypass(Residual):\n",
        "    def __init__(self, *args, dim=1):\n",
        "        return super().__init__(*args, reduce=lambda *t : torch.cat(t, dim=dim))\n",
        "\n",
        "class Sum(nn.ModuleList):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(args)\n",
        "      \n",
        "    def forward(self, input):\n",
        "        modules = list(self)\n",
        "        x = modules[0](input)\n",
        "        for module in modules[1:]:\n",
        "          x += module(input)\n",
        "        return x\n",
        "\n",
        "def Downsample(dim, dim_out = None):\n",
        "    return nn.Conv2d(dim, default(dim_out, dim), 4, 2, 1)\n",
        "\n",
        "def Upsample(dim, dim_out = None):\n",
        "    return nn.ConvTranspose2d(dim, default(dim_out, dim), 4, 2, 1)\n",
        "\n",
        "def Block(dim, dim_out, *, kernel_size=3, groups=8):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(dim, dim_out, kernel_size=kernel_size, padding=kernel_size//2),\n",
        "        nn.GroupNorm(groups, dim_out),\n",
        "        nn.SiLU(),\n",
        "    )\n",
        "\n",
        "def ResnetBlock(dim, dim_out, *, kernel_size=3, groups=8):\n",
        "    if dim != dim_out:\n",
        "        return Sum(\n",
        "            nn.Sequential(\n",
        "                Block(dim, dim_out, kernel_size=kernel_size, groups=groups),\n",
        "                Block(dim_out, dim_out, kernel_size=kernel_size, groups=groups),\n",
        "            ),\n",
        "            nn.Conv2d(dim, dim_out, 1),\n",
        "        )\n",
        "    else:\n",
        "        return Residual(\n",
        "            Block(dim, dim_out, kernel_size=kernel_size, groups=groups),\n",
        "            Block(dim_out, dim_out, kernel_size=kernel_size, groups=groups),\n",
        "        )\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, attention_dim=32):\n",
        "        super().__init__()\n",
        "        self.scale = attention_dim ** -0.5\n",
        "        self.heads = heads\n",
        "        self.to_kv = nn.Conv2d(dim, attention_dim * 2, 1, bias=False)\n",
        "        self.to_q = nn.Conv2d(dim, attention_dim * heads, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(attention_dim * heads, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        k, v = self.to_kv(x).chunk(2, dim=1)\n",
        "        q = einops.rearrange(self.to_q(x), 'b (h d) x y -> b h d x y', h=self.heads)\n",
        "\n",
        "        k = k.softmax(dim = 1)\n",
        "        q = q.softmax(dim = 2) * self.scale\n",
        "\n",
        "        context = torch.einsum('b d x y, b e x y -> b d e', k, v)\n",
        "        out = torch.einsum('b d e, b h d x y -> b e h x y', context, q)\n",
        "        out = einops.rearrange(out, 'b e h x y -> b (e h) x y')\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "class MemoryLookup(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    The core part of the MemoryAttention module, optimized for low memory usage.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, q, memory_k, module):\n",
        "        # Compute activations\n",
        "        logits = torch.einsum('n H q, H q m -> n H m', q, memory_k)\n",
        "\n",
        "        # Take top-k activations\n",
        "        logits_idx, indices = torch.topk(logits, module.k, dim=-1)\n",
        "        activations_idx = logits_idx.softmax(dim = -1)\n",
        "\n",
        "        ctx.save_for_backward(q, indices.clone())\n",
        "        ctx.module = module\n",
        "\n",
        "        return (activations_idx, indices)\n",
        "\n",
        "    @staticmethod \n",
        "    def scatter(input, dim, indices, new_size):\n",
        "        shape = list(input.shape)\n",
        "        shape[dim] = new_size\n",
        "        output = torch.zeros(shape, dtype=input.dtype, device=input.device)\n",
        "        output.scatter_(-1, indices, input)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_activations_idx, grad_indices):\n",
        "        q, indices = ctx.saved_tensors\n",
        "        module = ctx.module\n",
        "\n",
        "        # Rematerialize activations\n",
        "        logits = torch.einsum('n H q, H q m -> n H m', q, module.memory_k)\n",
        "        logits_idx = torch.gather(logits, -1, indices)\n",
        "        activations_idx = logits_idx.softmax(dim = -1)\n",
        "\n",
        "        # https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
        "        grad_logits_idx = grad_activations_idx * activations_idx - torch.einsum('n H m, n H m -> n H', grad_activations_idx, activations_idx).unsqueeze(-1) * activations_idx\n",
        "        grad_logits = MemoryLookup.scatter(grad_logits_idx, -1, indices, module.memory_size)\n",
        "\n",
        "        # grad_q = grad_logits * memory_k\n",
        "        # grad_memory_k = grad_logits * q\n",
        "        grad_q = torch.einsum('H q m, n H m -> n H q', module.memory_k, grad_logits)\n",
        "        grad_memory_k = torch.einsum('n H q, n H m -> H q m', q, grad_logits)\n",
        "\n",
        "        return (grad_q, grad_memory_k, None, None)\n",
        "\n",
        "class MemoryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Provides an attention-style memory that looks up output values based on a table.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, *, heads=4, key_dim=32, memory_dim=32, memory_size=1024, k=4, chunk_size=8192):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.key_dim = key_dim\n",
        "        self.memory_size = memory_size\n",
        "        self.memory_dim = memory_dim\n",
        "        self.k = k\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "        self.memory_k = nn.Parameter(torch.empty((heads, key_dim, memory_size)))\n",
        "        self.memory_v = nn.EmbeddingBag(memory_size * heads, embedding_dim=memory_dim, mode=\"sum\")\n",
        "        self.to_q = nn.Parameter(torch.empty((dim, heads, key_dim)))\n",
        "        self.to_out = nn.Parameter(torch.empty((heads, memory_dim, dim)))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        nn.init.uniform_(self.memory_k, -1, 1)\n",
        "        nn.init.uniform_(self.to_q, -1, 1)\n",
        "        nn.init.uniform_(self.to_out, -1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, w, h = x.shape\n",
        "        bwh = b * w * h\n",
        "\n",
        "        x = einops.rearrange(x, 'b c w h -> (b w h) c', b=b, c=c, w=w, h=h)\n",
        "\n",
        "        out = torch.zeros((bwh, self.dim), dtype=x.dtype, device=x.device)\n",
        "        for chunk_start in range(0, bwh, self.chunk_size):\n",
        "          chunk_end = min(chunk_start+self.chunk_size, bwh)\n",
        "          chunk_size = chunk_end - chunk_start\n",
        "\n",
        "          # Input to keys\n",
        "          q = torch.einsum('n c, c H k -> n H k', x[chunk_start:chunk_end], self.to_q)\n",
        "\n",
        "          # Compute activations\n",
        "          activations, indices = MemoryLookup.apply(q, self.memory_k, self)\n",
        "\n",
        "          head = torch.arange(0, self.heads, dtype=torch.long, device=x.device)\n",
        "          head = einops.rearrange(head, 'H -> 1 H 1')\n",
        "          indices += head * self.memory_size\n",
        "\n",
        "          # Weighted sum of top-k activations\n",
        "          activations = einops.rearrange(activations, 'n H m -> (n H) m')\n",
        "          indices = einops.rearrange(indices, 'n H m -> (n H) m')\n",
        "          v = self.memory_v(indices, per_sample_weights=activations)\n",
        "          v = einops.rearrange(v, '(n H) v -> n H v', H=self.heads)\n",
        "\n",
        "          # Values to output\n",
        "          out[chunk_start:chunk_end] = torch.einsum('n H v, H v c -> n c', v, self.to_out)\n",
        "        \n",
        "        out = einops.rearrange(out, '(b w h) c -> b c w h', b=b, c=c, w=w, h=h)\n",
        "        return out\n",
        "\n",
        "# Sanity check\n",
        "do_autograd_checks = False\n",
        "if do_autograd_checks:\n",
        "  test_memoryattention = MemoryAttention(4, heads=2, key_dim=4, memory_dim=4, memory_size=8, k=2)\n",
        "  test_memorylookup_input = torch.rand((test_memoryattention.dim, test_memoryattention.heads, test_memoryattention.key_dim), dtype=torch.float64)\n",
        "  test_memoryattention.memory_k.requires_grad = True\n",
        "  test_memoryattention.to(torch.float64)\n",
        "  test_memorylookup_input.requires_grad = True\n",
        "  torch.autograd.gradcheck(MemoryLookup.apply, (test_memorylookup_input, test_memoryattention.memory_k, test_memoryattention))\n",
        "  del test_memoryattention, test_memorylookup_input\n",
        "\n",
        "  torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "def LinearAttentionBlock(dim, attention_dim=32):\n",
        "    return Residual(\n",
        "        nn.InstanceNorm2d(dim, affine=True),\n",
        "        LinearAttention(dim, attention_dim=attention_dim),\n",
        "        nn.InstanceNorm2d(dim, affine=True),\n",
        "    )\n",
        "\n",
        "def MemoryAttentionBlock(dim, attention_dim=32):\n",
        "    return Residual(\n",
        "        nn.InstanceNorm2d(dim, affine=True),\n",
        "        MemoryAttention(dim, key_dim=attention_dim, memory_dim=attention_dim),\n",
        "        nn.InstanceNorm2d(dim, affine=True),\n",
        "    )\n",
        "\n",
        "class UNetBlock(nn.Sequential):\n",
        "    def __init__(self, ch, inner_ch, *inner_blocks):\n",
        "        super().__init__(\n",
        "            ResnetBlock(ch, ch),\n",
        "            ResnetBlock(ch, ch),\n",
        "            Bypass(\n",
        "                Downsample(ch, inner_ch),\n",
        "                *inner_blocks,\n",
        "                Upsample(inner_ch, ch),\n",
        "            ),\n",
        "            ResnetBlock(ch*2, ch),\n",
        "            ResnetBlock(ch, ch),\n",
        "        )\n",
        "\n",
        "#class SoftClamp(nn.Module):\n",
        "#    def __init__(self, min=None, max=None):\n",
        "#        super().__init__(self)\n",
        "#        self.min = min\n",
        "#        self.max = max\n",
        "#        self.register_full_backward_hook(self._backward_hook)\n",
        "#\n",
        "#    def forward(self, input):\n",
        "#        return input.clamp(-1, 1)\n",
        "#\n",
        "#    def _backward_hook(self, module, grad_input, grad_output):\n",
        "#        return grad_output\n",
        "#\n",
        "#    def extra_repr(self):\n",
        "#        return str(f\"min: {self.min}, max {self.max}\")\n",
        "\n",
        "# This acts like clamp(-1, 1) but passes through gradients unchanged\n",
        "#class SoftClampFn(torch.autograd.Function):\n",
        "#    staticmethod\n",
        "#    def forward(ctx, input):\n",
        "#        return input.clamp(-1, 1)\n",
        "#\n",
        "#    staticmethod\n",
        "#    def backward(ctx, grad_output):\n",
        "#        return grad_output\n",
        "#\n",
        "#softclamp = SoftClampFn.apply\n",
        "\n",
        "##############################################################################\n",
        "# Classes\n",
        "##############################################################################\n",
        "class GANLoss(nn.Module):\n",
        "    \"\"\"Define different GAN objectives.\n",
        "\n",
        "    The GANLoss class abstracts away the need to create the target label tensor\n",
        "    that has the same size as the input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0, device=None, dtype=torch.float):\n",
        "        \"\"\" Initialize the GANLoss class.\n",
        "\n",
        "        Parameters:\n",
        "            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n",
        "            target_real_label (bool) - - label for a real image\n",
        "            target_fake_label (bool) - - label of a fake image\n",
        "\n",
        "        Note: Do not use sigmoid as the last layer of Discriminator.\n",
        "        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n",
        "        \"\"\"\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label, device=device).to(dtype))\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label, device=device).to(dtype))\n",
        "        self.gan_mode = gan_mode\n",
        "        if gan_mode == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif gan_mode == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif gan_mode in ['wgangp']:\n",
        "            self.loss = None\n",
        "        else:\n",
        "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
        "\n",
        "    def get_target_tensor(self, prediction, target_is_real):\n",
        "        \"\"\"Create label tensors with the same size as the input.\n",
        "\n",
        "        Parameters:\n",
        "            prediction (tensor) - - tpyically the prediction from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "\n",
        "        Returns:\n",
        "            A label tensor filled with ground truth label, and with the size of the input\n",
        "        \"\"\"\n",
        "\n",
        "        if target_is_real:\n",
        "            target_tensor = self.real_label\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "        return target_tensor.expand_as(prediction)\n",
        "\n",
        "    def __call__(self, prediction, target_is_real):\n",
        "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
        "\n",
        "        Parameters:\n",
        "            prediction (tensor) - - typically the prediction output from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "\n",
        "        Returns:\n",
        "            the calculated loss.\n",
        "        \"\"\"\n",
        "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
        "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
        "            loss = self.loss(prediction, target_tensor)\n",
        "        elif self.gan_mode == 'wgangp':\n",
        "            if target_is_real:\n",
        "                loss = -prediction.mean()\n",
        "            else:\n",
        "                loss = prediction.mean()\n",
        "        return loss\n",
        "\n",
        "class Pix2PixModel:\n",
        "    \"\"\" This class implements the pix2pix model, for learning a mapping from input images to output images given paired data.\n",
        "\n",
        "    The model training requires '--dataset_mode aligned' dataset.\n",
        "    By default, it uses a '--netG unet256' U-Net generator,\n",
        "    a '--netD basic' discriminator (PatchGAN),\n",
        "    and a '--gan_mode' vanilla GAN loss (the cross-entropy objective used in the orignal GAN paper).\n",
        "\n",
        "    pix2pix paper: https://arxiv.org/pdf/1611.07004.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, opt):\n",
        "        \"\"\"Initialize the pix2pix class.\n",
        "\n",
        "        Parameters:\n",
        "            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
        "        \"\"\"\n",
        "        # Base model\n",
        "        self.opt = opt\n",
        "        self.isTrain = opt.isTrain\n",
        "        self.devices = opt.devices\n",
        "        self.dtype = opt.dtype\n",
        "        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)  # save all the checkpoints to save_dir\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        self.optimizers = []\n",
        "        self.image_paths = []\n",
        "        self.metric = 0  # used for learning rate policy 'plateau'\n",
        "\n",
        "        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n",
        "        self.loss_names = ['G_GAN', 'G_L1', 'D_real', 'D_fake']\n",
        "        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n",
        "        self.visual_names = ['real_A', 'fake_B', 'real_B']\n",
        "        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>\n",
        "        self.model_names = ['G']\n",
        "        # define networks (both generator and discriminator)\n",
        "        self.netG = nn.Sequential(\n",
        "            nn.Conv2d(opt.input_nc, opt.ngf, 7, padding=3),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            MemoryAttentionBlock(opt.ngf, opt.attention_dim),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            LinearAttentionBlock(opt.ngf, opt.attention_dim),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            MemoryAttentionBlock(opt.ngf, opt.attention_dim),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            LinearAttentionBlock(opt.ngf, opt.attention_dim),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            MemoryAttentionBlock(opt.ngf, opt.attention_dim),\n",
        "            nn.Conv2d(opt.ngf, opt.output_nc, 1),\n",
        "        )\n",
        "        self.netG = init_net(self.netG, opt.init_type, opt.init_gain, opt.devices, opt.dtype)\n",
        "\n",
        "        if self.isTrain:\n",
        "            # define loss functions\n",
        "            self.criterionL1 = torch.nn.L1Loss()\n",
        "            self.criterionMSE = torch.nn.MSELoss()\n",
        "            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n",
        "            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999), amsgrad=opt.amsgrad)\n",
        "            self.optimizers.append(self.optimizer_G)\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n",
        "        self.fake_B = self.netG(self.real_A)  # G(A)\n",
        "\n",
        "    def backward_G(self):\n",
        "        # Second, G(A) = B\n",
        "        self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1\n",
        "        self.loss_G_MSE = self.criterionMSE(self.fake_B, self.real_B) * self.opt.lambda_MSE\n",
        "        # combine loss and calculate gradients\n",
        "        self.loss_G = self.loss_G_L1 + self.loss_G_MSE\n",
        "        self.loss_G.backward()\n",
        "\n",
        "    def optimize_parameters(self):\n",
        "        self.forward()                   # compute fake images: G(A)\n",
        "        self.optimizer_G.zero_grad()        # set G's gradients to zero\n",
        "        self.backward_G()                   # calculate graidents for G\n",
        "        self.optimizer_G.step()             # udpate G's weights\n",
        "\n",
        "    def setup(self, opt):\n",
        "        \"\"\"Load and print networks; create schedulers\n",
        "\n",
        "        Parameters:\n",
        "            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
        "        \"\"\"\n",
        "        if self.isTrain:\n",
        "            self.schedulers = [get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n",
        "        self.print_networks(opt.verbose)\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Make models eval mode during test time\"\"\"\n",
        "        for name in self.model_names:\n",
        "            if isinstance(name, str):\n",
        "                net = getattr(self, 'net' + name)\n",
        "                net.eval()\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"Forward function used in test time.\n",
        "\n",
        "        This function wraps <forward> function in no_grad() so we don't save intermediate steps for backprop\n",
        "        It also calls <compute_visuals> to produce additional visualization results\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.forward()\n",
        "            self.compute_visuals()\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        \"\"\"Update learning rates for all the networks; called at the end of every epoch\"\"\"\n",
        "        old_lr = self.optimizers[0].param_groups[0]['lr']\n",
        "        for scheduler in self.schedulers:\n",
        "            scheduler.step()\n",
        "\n",
        "        lr = self.optimizers[0].param_groups[0]['lr']\n",
        "        print('learning rate %.7f -> %.7f' % (old_lr, lr))\n",
        "\n",
        "    def save_networks(self, epoch):\n",
        "        \"\"\"Save all the networks to the disk.\n",
        "\n",
        "        Parameters:\n",
        "            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n",
        "        \"\"\"\n",
        "        for name in self.model_names:\n",
        "            if isinstance(name, str):\n",
        "                save_filename = '%s_net_%s.pth' % (epoch, name)\n",
        "                save_path = os.path.join(self.save_dir, save_filename)\n",
        "                net = getattr(self, 'net' + name)\n",
        "                if isinstance(net, torch.nn.DataParallel):\n",
        "                    net = net.module\n",
        "                    \n",
        "                torch.save(net.state_dict(), save_path)\n",
        "\n",
        "    def load_networks(self, epoch):\n",
        "        \"\"\"Load all the networks from the disk.\n",
        "\n",
        "        Parameters:\n",
        "            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n",
        "        \"\"\"\n",
        "        for name in self.model_names:\n",
        "            if isinstance(name, str):\n",
        "                load_filename = '%s_net_%s.pth' % (epoch, name)\n",
        "                load_path = os.path.join(self.save_dir, load_filename)\n",
        "                net = getattr(self, 'net' + name)\n",
        "                if isinstance(net, torch.nn.DataParallel):\n",
        "                    net = net.module\n",
        "                print('loading the model from %s' % load_path)\n",
        "                # if you are using PyTorch newer than 0.4 (e.g., built from\n",
        "                # GitHub source), you can remove str() on self.device\n",
        "                state_dict = torch.load(load_path, map_location=self.devices[0])\n",
        "                if hasattr(state_dict, '_metadata'):\n",
        "                    del state_dict._metadata\n",
        "\n",
        "                net.load_state_dict(state_dict)\n",
        "\n",
        "    def print_networks(self, verbose):\n",
        "        \"\"\"Print the total number of parameters in the network and (if verbose) network architecture\n",
        "\n",
        "        Parameters:\n",
        "            verbose (bool) -- if verbose: print the network architecture\n",
        "        \"\"\"\n",
        "        print('---------- Networks initialized -------------')\n",
        "        for name in self.model_names:\n",
        "            if isinstance(name, str):\n",
        "                net = getattr(self, 'net' + name)\n",
        "                num_params = 0\n",
        "                for param in net.parameters():\n",
        "                    num_params += param.numel()\n",
        "                    if param.dtype != self.dtype:\n",
        "                      print(f\"Warning! Parameter has wrong dtype {param.dtype}, {param.device}, {param.numel()}. Expected {self.dtype}\")\n",
        "                    if str(param.device) != str(self.opt.devices[0]):\n",
        "                      print(f\"Warning! Parameter has wrong device {param.dtype}, {param.device}, {param.numel()}. Expected {self.opt.devices[0]}\")\n",
        "                for buffer in net.buffers():\n",
        "                    if buffer.dtype != self.dtype:\n",
        "                      print(f\"Warning! Buffer has wrong dtype {buffer.dtype}, {buffer.device}, {buffer.numel()}. Expected {self.dtype}\")\n",
        "                    if str(buffer.device) != str(self.opt.devices[0]):\n",
        "                      print(f\"Warning! Buffer has wrong device {buffer.dtype}, {buffer.device}, {buffer.numel()}. Expected {self.opt.devices[0]}\")\n",
        "                if verbose:\n",
        "                    print(net)\n",
        "                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n",
        "        print('-----------------------------------------------')\n",
        "\n",
        "    def set_requires_grad(self, nets, requires_grad=False):\n",
        "        \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
        "        Parameters:\n",
        "            nets (network list)   -- a list of networks\n",
        "            requires_grad (bool)  -- whether the networks require gradients or not\n",
        "        \"\"\"\n",
        "        if not isinstance(nets, list):\n",
        "            nets = [nets]\n",
        "        for net in nets:\n",
        "            if net is not None:\n",
        "                for param in net.parameters():\n",
        "                    param.requires_grad = requires_grad\n",
        "\n",
        "    def to(self, *args):\n",
        "      self.netG.to(*args)\n"
      ],
      "metadata": {
        "id": "iyjkOXSqXxld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model configuration"
      ],
      "metadata": {
        "id": "kQotN5mtzBUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class TrainOptions():\n",
        "    pass\n",
        "\n",
        "opt = TrainOptions()\n",
        "opt.checkpoints_dir = '/content/drive/MyDrive/AI/memorytest/checkpoints'\n",
        "opt.samples_dir = '/content/drive/MyDrive/AI/memorytest/samples'\n",
        "opt.name = \"memorytest1\"  #@param {type: \"string\"}\n",
        "\n",
        "#@markdown ## Model parameters\n",
        "#@markdown **Changing these will invalidate checkpoint files!**\n",
        "opt.input_nc = opt.output_nc = 3\n",
        "opt.ngf = 64              #@param {type: \"integer\"}\n",
        "opt.attention_dim =     64#@param {type: \"integer\"}\n",
        "\n",
        "#@markdown ## Training parameters\n",
        "opt.init_type = \"normal\"\n",
        "opt.init_gain = 0.02\n",
        "opt.beta1 = 0.9           #@param {type: \"number\"}\n",
        "opt.lr = 0.0002           #@param {type: \"number\"}\n",
        "opt.amsgrad = True        #@param {type: \"boolean\"}\n",
        "opt.epoch_count = 1\n",
        "opt.n_epochs = 100\n",
        "opt.n_epochs_decay = 100\n",
        "opt.batch_size =         4#@param {type: \"integer\"}\n",
        "opt.noise_schedule_beta = 0.003   #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ## Objective parameters\n",
        "opt.gan_mode = \"lsgan\"    #@param [\"vanilla\", \"lsgan\", \"wgangp\"]\n",
        "opt.lambda_L1 = 5       #@param {type: \"number\"}\n",
        "opt.lambda_MSE = 25    #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ## Debugging\n",
        "opt.verbose = False       #@param {type: 'boolean'}\n",
        "opt.display_interval = 25 #@param {type: \"integer\"}\n",
        "opt.debug_xla = False      #@param {type: 'boolean'}\n",
        "\n",
        "#@markdown ## Backend selection\n",
        "opt.backend = 'CUDA' #@param [\"CPU\", \"CUDA\", \"TPU\"]\n",
        "opt.dtype = 'float32' #@param [\"float32\", \"bfloat16\"]\n",
        "\n",
        "if opt.backend == 'CUDA':\n",
        "  torch.cuda.set_device(0)\n",
        "  opt.devices = []\n",
        "  for i in range(0, torch.cuda.device_count()):\n",
        "    opt.devices.append(torch.device(f'cuda:{i}'))\n",
        "elif opt.backend == 'TPU':\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  opt.devices = [xm.xla_device()]\n",
        "else:\n",
        "  opt.devices = [torch.device('cpu')]\n",
        "\n",
        "if opt.dtype == \"bfloat16\":\n",
        "  opt.dtype = torch.bfloat16\n",
        "else:\n",
        "  opt.dtype = torch.float\n",
        "\n"
      ],
      "metadata": {
        "id": "o4iRw1MpzDvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "AlfZaNA3XqY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, random\n",
        "import torch\n",
        "import torchvision as tv\n",
        "import einops\n",
        "import multiprocessing\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torchvision.datasets import ImageFolder\n",
        "from IPython import display\n",
        "\n",
        "dataset = tv.datasets.ImageFolder(root = DATA_DIR, \n",
        "  transform = tv.transforms.Compose([\n",
        "    #tv.transforms.Resize(256),                                                            \n",
        "    tv.transforms.ToTensor(),\n",
        "  ]))\n",
        "if opt.backend == 'TPU':\n",
        "  sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "  data_loader = torch.utils.data.DataLoader(dataset,\n",
        "    sampler=sampler,\n",
        "    batch_size=opt.batch_size,\n",
        "    num_workers=4,\n",
        "    drop_last=True)\n",
        "else:\n",
        "  data_loader = torch.utils.data.DataLoader(dataset,\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    drop_last=True)\n",
        "\n",
        "\n",
        "opt.isTrain = True\n",
        "opt.continue_train = False\n",
        "\n",
        "RUN_DIR = f\"{opt.checkpoints_dir}/{opt.name}\"\n",
        "!mkdir -p $RUN_DIR\n",
        "\n",
        "model = Pix2PixModel(opt)\n",
        "model.setup(opt)\n",
        "\n",
        "start_epoch = 1\n",
        "last_epoch = 10\n",
        "# Resume\n",
        "try:\n",
        "  cp = open(f\"{opt.checkpoints_dir}/{opt.name}/latest_epoch\", \"r\")\n",
        "  start_epoch = int(cp.read())\n",
        "  model.load_networks(str(start_epoch))\n",
        "  start_epoch += 1\n",
        "  cp.close()\n",
        "except:\n",
        "  print(f\"Failed to load checkpoint {start_epoch}\")\n",
        "  start_epoch = 1\n",
        "  pass\n",
        "\n",
        "arch = open(f\"{opt.checkpoints_dir}/{opt.name}/architecture\", \"w\")\n",
        "arch.write(f\"netG: {model.netG}\\n\")\n",
        "arch.close()\n",
        "\n",
        "#display.clear_output(wait=True)\n",
        "#dataset[1000][0]\n",
        "\n",
        "crop = tv.transforms.RandomCrop(256)\n",
        "\n",
        "colab_display = True\n",
        "epochs = range(start_epoch, last_epoch+1)\n",
        "try:\n",
        "  epochs = tqdm_notebook(epochs, desc='Epoch')\n",
        "except:\n",
        "  colab_display = False\n",
        "\n",
        "for epoch in epochs:\n",
        "  model.update_learning_rate()\n",
        "\n",
        "  batches = enumerate(data_loader)\n",
        "  try:\n",
        "    batches = tqdm_notebook(enumerate(data_loader), desc='Batch', total=len(data_loader))\n",
        "  except:\n",
        "    colab_display = False\n",
        "\n",
        "  for i, data in batches:\n",
        "    output_data = crop(data[0].to(opt.dtype).to(opt.devices[0])) * 2 - 1\n",
        "    time = torch.rand(size = (output_data.size()[0], ), device=opt.devices[0], dtype=opt.dtype) * 1000\n",
        "    beta = einops.rearrange((1 - opt.noise_schedule_beta) ** time, 'd -> d 1 1 1')\n",
        "    input_data = (output_data * torch.sqrt(1 - beta ** 2) + torch.randn_like(output_data) * beta)\n",
        "\n",
        "    model.real_A = input_data.to(opt.dtype)\n",
        "    model.real_B = output_data.to(opt.dtype)\n",
        "\n",
        "    model.optimize_parameters()\n",
        "\n",
        "    if opt.backend == 'TPU' and opt.debug_xla:\n",
        "      print(torch_xla._XLAC._get_xla_tensors_text([model.loss_G_L1, model.loss_G_MSE]))\n",
        "\n",
        "    if opt.display_interval > 0 and i % opt.display_interval == 0:\n",
        "      if colab_display:\n",
        "        display.clear_output(wait=True)\n",
        "        model.print_networks(verbose=opt.verbose)\n",
        "        display.display(epochs.container)\n",
        "        display.display(batches.container)\n",
        "      #print(f\"epoch {epoch}/{last_epoch} batch {i}/{len(data_loader)}\")\n",
        "      if opt.backend != 'TPU' or True:\n",
        "        sample_index = 0\n",
        "        sample = to_pil_image(torch.cat((model.real_A[sample_index],\n",
        "          model.real_B[sample_index],\n",
        "          model.fake_B[sample_index]), dim=2).clamp(-1, 1) * 0.5 + 0.5)\n",
        "        print(f\"Sample {sample_index}: time {time[sample_index]}, beta {beta[sample_index][0][0][0]}\")\n",
        "        display.display_png(sample)\n",
        "      print(f\"Batch losses: G_L1 {model.loss_G_L1}, G_MSE {model.loss_G_MSE}\")\n",
        "  \n",
        "  model.save_networks('latest')\n",
        "  model.save_networks(epoch)\n",
        "  cp = open(f\"{opt.checkpoints_dir}/{opt.name}/latest_epoch\", \"w\")\n",
        "  cp.write(str(epoch))\n",
        "\n",
        "  cp.close()\n"
      ],
      "metadata": {
        "id": "3dNW0jxvb60P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denoise"
      ],
      "metadata": {
        "id": "cPM77ZOm4uST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, io, requests\n",
        "import torch\n",
        "import einops\n",
        "from PIL import Image\n",
        "import torchvision as tv\n",
        "from torchvision.transforms.functional import to_tensor, to_pil_image, resize\n",
        "from IPython import display\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "\n",
        "image_url = \"https://i.imgur.com/8DDxOuy.png\" #@param {type: \"string\"}\n",
        "image_size = [256,256] #@param {type: \"raw\"}\n",
        "noise_beta =  0#@param {type: \"number\"}\n",
        "iterations =  1#@param {type: \"integer\", min:1}\n",
        "\n",
        "#@markdown ## Network layer visualization\n",
        "#@markdown Enter a submodule id (e.g. \"3.2\") here to see a visualization of the activations for that layer.\n",
        "#@markdown You can also enter multiple ids separated by spaces.\n",
        "visualize_layers = False #@param {type: \"boolean\"}\n",
        "visualize_pca = 8 #@param {type: \"integer\"}\n",
        "visualize_as_overlay = True #@param {type: \"boolean\"}\n",
        "display_visualization = False #@param {type: \"boolean\"}\n",
        "save_visualization = True #@param {type: \"boolean\"}\n",
        "\n",
        "SAMPLES_DIR = f\"{opt.samples_dir}/{opt.name}\"\n",
        "!mkdir -p $SAMPLES_DIR\n",
        "\n",
        "def fetch(url_or_path):\n",
        "  if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "    r = requests.get(url_or_path)\n",
        "    r.raise_for_status()\n",
        "    fd = io.BytesIO()\n",
        "    fd.write(r.content)\n",
        "    fd.seek(0)\n",
        "    return fd\n",
        "  return open(url_or_path, 'rb')\n",
        "\n",
        "def PCA(X, k):\n",
        "  \"\"\"\n",
        "  Perform principal components analysis on X and return the top k components.\n",
        "  \"\"\"\n",
        "  X_center = X - torch.mean(X, dim=0, keepdim=True)\n",
        "  cov = torch.cov(X_center, correction=0)\n",
        "  values, vectors = torch.linalg.eig(cov)\n",
        "  output = torch.einsum('c v, c i -> v i', vectors[:,:k].real, X_center)\n",
        "  return output\n",
        "\n",
        "opt.isTrain = False\n",
        "\n",
        "model = Pix2PixModel(opt)\n",
        "model.setup(opt)\n",
        "model.load_networks('latest')\n",
        "\n",
        "class HookBuffer:\n",
        "  def __init__(self, layer_id):\n",
        "    self.buffer = None\n",
        "    self.layer_id = layer_id\n",
        "\n",
        "  def hook(self, module, input, output):\n",
        "    self.buffer = output\n",
        "    self.module_name = type(module).__name__\n",
        "    self.seq = HookBuffer.buffer_seq\n",
        "    HookBuffer.buffer_seq += 1\n",
        "\n",
        "HookBuffer.buffer_seq = 0\n",
        "vis_buffers = []\n",
        "if visualize_layers:\n",
        "  try:\n",
        "    main_module = model.netG.get_submodule('module')\n",
        "  except:\n",
        "    main_module = model.netG\n",
        "  for layer_id, submodule in main_module.named_modules():\n",
        "    if (type(submodule) == nn.Sequential or \n",
        "        type(submodule) == Bypass or\n",
        "        type(submodule) == UNetBlock or\n",
        "        type(submodule) == nn.GroupNorm or\n",
        "        type(submodule) == nn.InstanceNorm2d or\n",
        "        type(submodule) == nn.Conv2d or\n",
        "        type(submodule) == nn.ConvTranspose2d):\n",
        "      continue\n",
        "    buffer = HookBuffer(layer_id)\n",
        "    submodule.register_forward_hook(buffer.hook)\n",
        "    vis_buffers.append(buffer)\n",
        "\n",
        "if image_url != \"\":\n",
        "  pil_image_input = Image.open(fetch(image_url)).convert('RGB')\n",
        "\n",
        "  if image_size != None:\n",
        "    pil_image_input = resize(pil_image_input, image_size)\n",
        "\n",
        "  print(\"Input image\")\n",
        "  display.display_png(pil_image_input)\n",
        "\n",
        "  image_input = to_tensor(pil_image_input).to(model.devices[0])\n",
        "  image_input = einops.rearrange(image_input, '... -> 1 ...')\n",
        "  image_input = image_input * 2 - 1\n",
        "else:\n",
        "  if image_size == None:\n",
        "    image_size = [256, 256]\n",
        "  image_input = torch.zeros(1, 3, image_size[0], image_size[1]).to(model.devices[0])\n",
        "\n",
        "image_result = image_input * math.sqrt(1 - noise_beta ** 2) + torch.randn_like(image_input) * noise_beta\n",
        "\n",
        "model.set_requires_grad(model.netG, False)\n",
        "\n",
        "cum_beta = noise_beta\n",
        "for step in tqdm_notebook(range(0, iterations), desc='Iteration'):\n",
        "  step_beta = 1 / (iterations - step)\n",
        "\n",
        "  iter = model.netG(image_result)\n",
        "  image_result = image_result * math.sqrt(1 - step_beta ** 2) + iter * (step_beta)\n",
        "  cum_beta *= (1 - step_beta)\n",
        "\n",
        "print(\"Final result\")\n",
        "display.display_png(to_pil_image(image_result[0].clamp(-1, 1) * 0.5 + 0.5))\n",
        "\n",
        "for buffer in tqdm_notebook(vis_buffers, desc='Processing layer'):\n",
        "  activations = nn.functional.instance_norm(buffer.buffer)\n",
        "\n",
        "  if visualize_pca > 0:\n",
        "    _, c, w, h = activations.shape\n",
        "    activations = einops.rearrange(activations, '1 c w h -> c (w h)')\n",
        "    activations = PCA(activations, visualize_pca)\n",
        "    activations = einops.rearrange(activations, 'c (w h) -> c 1 w h', w=w, h=h)\n",
        "  else:\n",
        "    activations = einops.rearrange(activations, '1 c w h -> c 1 w h')\n",
        "\n",
        "  red = einops.repeat(torch.tensor([1, 0, 0]), 'c -> 1 c 1 1').to(model.devices[0])\n",
        "  blue = einops.repeat(torch.tensor([0, 0, 1]), 'c -> 1 c 1 1').to(model.devices[0])\n",
        "\n",
        "  chunks = math.ceil(activations.shape[0] / 64)\n",
        "  chunkid = 0\n",
        "  for chunk in torch.chunk(activations, chunks, dim=0):\n",
        "    channels = chunk.clone()\n",
        "    a = channels.shape[0]\n",
        "    nrow = max(math.floor(math.sqrt(a)), 4)\n",
        "\n",
        "    _, c, w, h = image_input.shape\n",
        "\n",
        "    image_fade = 0.5\n",
        "    channels /= nn.functional.adaptive_max_pool2d(channels.abs(), output_size=1)\n",
        "    #channels = einops.repeat(channels, 'a 1 x y -> a c x y', c=c)\n",
        "    if visualize_as_overlay:\n",
        "      channels = tv.transforms.functional.resize(\n",
        "          channels, (w, h), \n",
        "          interpolation=tv.transforms.functional.InterpolationMode.NEAREST)\n",
        "      channels = (\n",
        "          (image_input * image_fade * 0.5 + 0.5) * (1 - channels.abs()).clamp(min=0) + \n",
        "          red * (channels).clamp(min=0) +\n",
        "          blue * (-channels).clamp(min=0))\n",
        "    else:\n",
        "      channels = (\n",
        "          red * (channels).clamp(min=0) +\n",
        "          blue * (-channels).clamp(min=0))\n",
        "      \n",
        "    grid = tv.utils.make_grid(channels, nrow=nrow, normalize=False)\n",
        "    pil_image_layers = to_pil_image(grid)\n",
        "    if display_visualization:\n",
        "      print(f'Layer {buffer.layer_id} ({buffer.module_name})')\n",
        "      display.display_png(pil_image_layers)\n",
        "    if save_visualization:\n",
        "      filename = f\"{opt.samples_dir}/{opt.name}/layer{buffer.seq}_{buffer.module_name}_{buffer.layer_id}_{chunkid}.png\"\n",
        "      pil_image_layers.save(filename)\n",
        "      #print(f'Saved \"{filename}\"')\n",
        "    chunkid += 1\n"
      ],
      "metadata": {
        "id": "6fVpqu5u4vlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = False\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "data = torch.randn([4, 256, 256, 256], dtype=torch.float, device='cuda', requires_grad=True)\n",
        "net = torch.nn.Conv2d(256, 64, kernel_size=[1, 1], padding=[0, 0], stride=[1, 1], dilation=[1, 1], groups=1)\n",
        "net = net.cuda().float()\n",
        "out = net(data)\n",
        "out.backward(torch.randn_like(out))\n",
        "torch.cuda.synchronize()"
      ],
      "metadata": {
        "id": "eoxHdfgxy8aM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}