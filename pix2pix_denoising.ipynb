{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pix2pix denoising.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "private_outputs": true,
      "authorship_tag": "ABX9TyMDyGxmw+tJYHrD4hXmyD5q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RossM/machine-learning-colabs/blob/main/pix2pix_denoising.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "IfXEFmRvYdmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU status"
      ],
      "metadata": {
        "id": "Qv1x14G-ZjeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "simple_nvidia_smi_display = False#@param {type:\"boolean\"}\n",
        "if simple_nvidia_smi_display:\n",
        "    #!nvidia-smi\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_output)\n",
        "else:\n",
        "    #!nvidia-smi -i 0 -e 0\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_output)\n",
        "    nvidiasmi_ecc_note = subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_ecc_note)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WIdM1Kn3YcPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Google Drive"
      ],
      "metadata": {
        "id": "DoExgck_ZZBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4TGsnF08aWc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the data"
      ],
      "metadata": {
        "id": "jakoyk_jZWH3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwvZkapDUhlJ"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"/content/danbooru2021/images\"\n",
        "\n",
        "!mkdir -p $DATA_DIR\n",
        "!rsync --progress --recursive --size-only --verbose rsync://176.9.41.242:873/danbooru2021/512px/000*/ $DATA_DIR/512px/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "pq_PSpPyZT8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  !pip install cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install dominate wandb torchvision einops denoising-diffusion-pytorch"
      ],
      "metadata": {
        "id": "dILRNjf-axy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define our model"
      ],
      "metadata": {
        "id": "ZbbGfN5dZQ4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, functools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import einops\n",
        "from torch.nn import init\n",
        "from torch.optim import lr_scheduler\n",
        "import denoising_diffusion_pytorch.denoising_diffusion_pytorch as dd\n",
        "\n",
        "# Based on code from pytorch-pix2pix https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
        "\n",
        "def default(val, d):\n",
        "    if val is not None:\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "def init_weights(net, init_type='normal', init_gain=0.02):\n",
        "    \"\"\"Initialize network weights.\n",
        "\n",
        "    Parameters:\n",
        "        net (network)   -- network to be initialized\n",
        "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
        "    work better for some applications. Feel free to try yourself.\n",
        "    \"\"\"\n",
        "    def init_func(m):  # define the initialization function\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "            if init_type == 'normal':\n",
        "                init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
        "            init.normal_(m.weight.data, 1.0, init_gain)\n",
        "            init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    print('initialize network with %s' % init_type)\n",
        "    net.apply(init_func)  # apply the initialization function <init_func>\n",
        "\n",
        "\n",
        "def init_net(net, init_type='normal', init_gain=0.02, devices=[\"cpu\"], dtype=torch.float):\n",
        "    \"\"\"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n",
        "    Parameters:\n",
        "        net (network)      -- the network to be initialized\n",
        "        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "        gain (float)       -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    Return an initialized network.\n",
        "    \"\"\"\n",
        "    net.to(dtype)\n",
        "    if devices[0] != \"cpu\":\n",
        "        net.to(devices[0])\n",
        "        net = torch.nn.DataParallel(net, devices)  # multi-GPUs\n",
        "    init_weights(net, init_type, init_gain=init_gain)\n",
        "    return net\n",
        "\n",
        "def get_scheduler(optimizer, opt):\n",
        "    \"\"\"Return a learning rate scheduler\n",
        "\n",
        "    Parameters:\n",
        "        optimizer          -- the optimizer of the network\n",
        "        opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions．　\n",
        "                              opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\n",
        "\n",
        "    For 'linear', we keep the same learning rate for the first <opt.n_epochs> epochs\n",
        "    and linearly decay the rate to zero over the next <opt.n_epochs_decay> epochs.\n",
        "    For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.\n",
        "    See https://pytorch.org/docs/stable/optim.html for more details.\n",
        "    \"\"\"\n",
        "    def lambda_rule(epoch):\n",
        "        lr_l = 1.0 - max(0, epoch + opt.epoch_count - opt.n_epochs) / float(opt.n_epochs_decay + 1)\n",
        "        return lr_l\n",
        "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
        "    return scheduler\n",
        "\n",
        "##############################################################################\n",
        "# NN-Blocks\n",
        "##############################################################################\n",
        "\n",
        "class SequentialExt(nn.Sequential):\n",
        "    def forward(self, *args):\n",
        "        modules = list(self)\n",
        "        input = modules[0](*args)\n",
        "        for module in modules[1:]:\n",
        "            input = module(input)\n",
        "        return input\n",
        "\n",
        "def nameof(obj):\n",
        "    if hasattr(obj, '__name__'):\n",
        "        return obj.__name__\n",
        "    return str(obj)\n",
        "\n",
        "class Residual(nn.Sequential):\n",
        "    def __init__(self, *args, reduce=None):\n",
        "        super().__init__(*args)\n",
        "        self.reduce = reduce\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = input\n",
        "        for module in self:\n",
        "            x = module(x)\n",
        "        if self.reduce != None:\n",
        "            return self.reduce(x, input)\n",
        "        else:\n",
        "            return x + input\n",
        "\n",
        "    def extra_repr(self):\n",
        "        if self.reduce != None:\n",
        "            return str(f\"reduce={nameof(self.reduce)}\")\n",
        "\n",
        "def Bypass(*args, dim=1):\n",
        "    def cat(*tensors):\n",
        "        return torch.cat(tensors, dim=dim)\n",
        "    return Residual(*args, reduce=cat)\n",
        "\n",
        "class Sum(nn.ModuleList):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(args)\n",
        "      \n",
        "    def forward(self, input):\n",
        "        modules = list(self)\n",
        "        x = modules[0](input)\n",
        "        for module in modules[1:]:\n",
        "          x += module(input)\n",
        "        return x\n",
        "\n",
        "def Downsample(dim, dim_out = None):\n",
        "    return nn.Conv2d(dim, default(dim_out, dim), 4, 2, 1)\n",
        "\n",
        "def Upsample(dim, dim_out = None):\n",
        "    return nn.ConvTranspose2d(dim, default(dim_out, dim), 4, 2, 1)\n",
        "\n",
        "def Block(dim, dim_out, *, kernel_size=3, groups=8):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(dim, dim_out, kernel_size=kernel_size, padding=kernel_size//2),\n",
        "        nn.GroupNorm(groups, dim_out),\n",
        "        nn.SiLU(),\n",
        "    )\n",
        "\n",
        "def ResnetBlock(dim, dim_out, *, kernel_size=3, groups=8):\n",
        "    if dim != dim_out:\n",
        "        return Sum(\n",
        "            nn.Sequential(\n",
        "                Block(dim, dim_out, kernel_size=kernel_size, groups=groups),\n",
        "                Block(dim_out, dim_out, kernel_size=kernel_size, groups=groups),\n",
        "            ),\n",
        "            nn.Conv2d(dim, dim_out, 1),\n",
        "        )\n",
        "    else:\n",
        "        return Residual(\n",
        "            Block(dim, dim_out, kernel_size=kernel_size, groups=groups),\n",
        "            Block(dim_out, dim_out, kernel_size=kernel_size, groups=groups),\n",
        "        )\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, attention_dim=32):\n",
        "        super().__init__()\n",
        "        self.scale = attention_dim ** -0.5\n",
        "        self.heads = heads\n",
        "        self.to_kv = nn.Conv2d(dim, attention_dim * 2, 1, bias=False)\n",
        "        self.to_q = nn.Conv2d(dim, attention_dim * heads, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(attention_dim * heads, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        k, v = self.to_kv(x).chunk(2, dim=1)\n",
        "        q = einops.rearrange(self.to_q(x), 'b (h d) x y -> b h d x y', h=self.heads)\n",
        "\n",
        "        k = k.softmax(dim = 1)\n",
        "        q = q.softmax(dim = 2) * self.scale\n",
        "\n",
        "        context = torch.einsum('b d x y, b e x y -> b d e', k, v)\n",
        "        out = torch.einsum('b d e, b h d x y -> b h e x y', context, q)\n",
        "        out = einops.rearrange(out, 'b h e x y -> b (h e) x y')\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "def Attention(dim, attention_dim=32):\n",
        "    return Residual(\n",
        "        nn.InstanceNorm2d(dim, affine=True),\n",
        "        LinearAttention(dim, attention_dim=attention_dim),\n",
        "        nn.InstanceNorm2d(dim, affine=True),\n",
        "    )\n",
        "\n",
        "class UNetBlock(nn.Sequential):\n",
        "    @staticmethod\n",
        "    def cat(x, y):\n",
        "        return torch.cat((x, y), dim=1)\n",
        "\n",
        "    def __init__(self, ch, inner_ch, attention_dim, *inner_blocks):\n",
        "        super().__init__(\n",
        "            ResnetBlock(ch, ch),\n",
        "            ResnetBlock(ch, ch),\n",
        "            Residual(\n",
        "                Downsample(ch, inner_ch),\n",
        "                Attention(inner_ch, attention_dim),\n",
        "                *inner_blocks,\n",
        "                Upsample(inner_ch, ch),\n",
        "                reduce=self.cat\n",
        "            ),\n",
        "            #nn.Conv2d(ch*2, ch, 1),\n",
        "            ResnetBlock(ch*2, ch),\n",
        "            ResnetBlock(ch, ch),\n",
        "            Attention(ch, attention_dim),\n",
        "        )\n",
        "\n",
        "\n",
        "#class SoftClamp(nn.Module):\n",
        "#    def __init__(self, min=None, max=None):\n",
        "#        super().__init__(self)\n",
        "#        self.min = min\n",
        "#        self.max = max\n",
        "#        self.register_full_backward_hook(self._backward_hook)\n",
        "#\n",
        "#    def forward(self, input):\n",
        "#        return input.clamp(-1, 1)\n",
        "#\n",
        "#    def _backward_hook(self, module, grad_input, grad_output):\n",
        "#        return grad_output\n",
        "#\n",
        "#    def extra_repr(self):\n",
        "#        return str(f\"min: {self.min}, max {self.max}\")\n",
        "\n",
        "# This acts like clamp(-1, 1) but passes through gradients unchanged\n",
        "#class SoftClampFn(torch.autograd.Function):\n",
        "#    staticmethod\n",
        "#    def forward(ctx, input):\n",
        "#        return input.clamp(-1, 1)\n",
        "#\n",
        "#    staticmethod\n",
        "#    def backward(ctx, grad_output):\n",
        "#        return grad_output\n",
        "#\n",
        "#softclamp = SoftClampFn.apply\n",
        "\n",
        "##############################################################################\n",
        "# Classes\n",
        "##############################################################################\n",
        "class GANLoss(nn.Module):\n",
        "    \"\"\"Define different GAN objectives.\n",
        "\n",
        "    The GANLoss class abstracts away the need to create the target label tensor\n",
        "    that has the same size as the input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0, device=None, dtype=torch.float):\n",
        "        \"\"\" Initialize the GANLoss class.\n",
        "\n",
        "        Parameters:\n",
        "            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n",
        "            target_real_label (bool) - - label for a real image\n",
        "            target_fake_label (bool) - - label of a fake image\n",
        "\n",
        "        Note: Do not use sigmoid as the last layer of Discriminator.\n",
        "        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n",
        "        \"\"\"\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(target_real_label, device=device).to(dtype))\n",
        "        self.register_buffer('fake_label', torch.tensor(target_fake_label, device=device).to(dtype))\n",
        "        self.gan_mode = gan_mode\n",
        "        if gan_mode == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif gan_mode == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif gan_mode in ['wgangp']:\n",
        "            self.loss = None\n",
        "        else:\n",
        "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
        "\n",
        "    def get_target_tensor(self, prediction, target_is_real):\n",
        "        \"\"\"Create label tensors with the same size as the input.\n",
        "\n",
        "        Parameters:\n",
        "            prediction (tensor) - - tpyically the prediction from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "\n",
        "        Returns:\n",
        "            A label tensor filled with ground truth label, and with the size of the input\n",
        "        \"\"\"\n",
        "\n",
        "        if target_is_real:\n",
        "            target_tensor = self.real_label\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "        return target_tensor.expand_as(prediction)\n",
        "\n",
        "    def __call__(self, prediction, target_is_real):\n",
        "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
        "\n",
        "        Parameters:\n",
        "            prediction (tensor) - - typically the prediction output from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "\n",
        "        Returns:\n",
        "            the calculated loss.\n",
        "        \"\"\"\n",
        "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
        "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
        "            loss = self.loss(prediction, target_tensor)\n",
        "        elif self.gan_mode == 'wgangp':\n",
        "            if target_is_real:\n",
        "                loss = -prediction.mean()\n",
        "            else:\n",
        "                loss = prediction.mean()\n",
        "        return loss\n",
        "\n",
        "class Pix2PixModel:\n",
        "    \"\"\" This class implements the pix2pix model, for learning a mapping from input images to output images given paired data.\n",
        "\n",
        "    The model training requires '--dataset_mode aligned' dataset.\n",
        "    By default, it uses a '--netG unet256' U-Net generator,\n",
        "    a '--netD basic' discriminator (PatchGAN),\n",
        "    and a '--gan_mode' vanilla GAN loss (the cross-entropy objective used in the orignal GAN paper).\n",
        "\n",
        "    pix2pix paper: https://arxiv.org/pdf/1611.07004.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, opt):\n",
        "        \"\"\"Initialize the pix2pix class.\n",
        "\n",
        "        Parameters:\n",
        "            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
        "        \"\"\"\n",
        "        # Base model\n",
        "        self.opt = opt\n",
        "        self.isTrain = opt.isTrain\n",
        "        self.devices = opt.devices\n",
        "        self.dtype = opt.dtype\n",
        "        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)  # save all the checkpoints to save_dir\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        self.optimizers = []\n",
        "        self.image_paths = []\n",
        "        self.metric = 0  # used for learning rate policy 'plateau'\n",
        "\n",
        "        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n",
        "        self.loss_names = ['G_GAN', 'G_L1', 'D_real', 'D_fake']\n",
        "        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n",
        "        self.visual_names = ['real_A', 'fake_B', 'real_B']\n",
        "        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>\n",
        "        if self.isTrain:\n",
        "            self.model_names = ['G', 'D']\n",
        "        else:  # during test time, only load G\n",
        "            self.model_names = ['G']\n",
        "        # define networks (both generator and discriminator)\n",
        "        self.netG = nn.Sequential(\n",
        "            nn.Conv2d(opt.input_nc, opt.ngf, 7, padding=3),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            Attention(opt.ndf, opt.attention_dim),\n",
        "            UNetBlock(opt.ngf, opt.ngf * 2, opt.attention_dim,\n",
        "                UNetBlock(opt.ngf * 2, opt.ngf * 4, opt.attention_dim,\n",
        "                    UNetBlock(opt.ngf * 4, opt.ngf * 8, opt.attention_dim,\n",
        "                        ResnetBlock(opt.ngf * 8, opt.ngf * 8),\n",
        "                        ResnetBlock(opt.ngf * 8, opt.ngf * 8),\n",
        "                        Attention(opt.ngf * 8, opt.attention_dim),\n",
        "                        ResnetBlock(opt.ngf * 8, opt.ngf * 8),\n",
        "                        ResnetBlock(opt.ngf * 8, opt.ngf * 8),\n",
        "                        Attention(opt.ngf * 8, opt.attention_dim),\n",
        "                    )\n",
        "                )\n",
        "            ),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            ResnetBlock(opt.ngf, opt.ngf),\n",
        "            nn.Conv2d(opt.ngf, opt.output_nc, 1)\n",
        "        )\n",
        "        self.netG = init_net(self.netG, opt.init_type, opt.init_gain, opt.devices, opt.dtype)\n",
        "\n",
        "        if self.isTrain:  # define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc\n",
        "            self.netD = nn.Sequential(\n",
        "                nn.Conv2d(opt.input_nc+opt.output_nc, opt.ndf, 7, padding=3),\n",
        "                ResnetBlock(opt.ndf, opt.ndf, kernel_size=7),\n",
        "                ResnetBlock(opt.ndf, opt.ndf, kernel_size=7),\n",
        "                Attention(opt.ndf, opt.attention_dim),\n",
        "                ResnetBlock(opt.ndf, opt.ndf, kernel_size=7),\n",
        "                ResnetBlock(opt.ndf, opt.ndf, kernel_size=7),\n",
        "                Downsample(opt.ndf, opt.ndf*2),\n",
        "                Attention(opt.ndf*2, opt.attention_dim),\n",
        "                ResnetBlock(opt.ndf*2, opt.ndf*2),\n",
        "                ResnetBlock(opt.ndf*2, opt.ndf*2),\n",
        "                Downsample(opt.ndf*2, opt.ndf*4),\n",
        "                Attention(opt.ndf*4, opt.attention_dim),\n",
        "                ResnetBlock(opt.ndf*4, opt.ndf*4),\n",
        "                ResnetBlock(opt.ndf*4, opt.ndf*4),\n",
        "                Downsample(opt.ndf*4, opt.ndf*8),\n",
        "                Attention(opt.ndf*8, opt.attention_dim),\n",
        "                ResnetBlock(opt.ndf*8, opt.ndf*8),\n",
        "                ResnetBlock(opt.ndf*8, opt.ndf*8),\n",
        "                Attention(opt.ndf*8, opt.attention_dim),\n",
        "                nn.Conv2d(opt.ndf*8, opt.ndf*8, 1),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(opt.ndf*8, 1, 1),\n",
        "            )\n",
        "            self.netD = init_net(self.netD, opt.init_type, opt.init_gain, opt.devices, opt.dtype)\n",
        "\n",
        "        if self.isTrain:\n",
        "            # define loss functions\n",
        "            self.criterionGAN = GANLoss(opt.gan_mode, device=self.devices[0], dtype=self.dtype)\n",
        "            self.criterionL1 = torch.nn.L1Loss()\n",
        "            self.criterionMSE = torch.nn.MSELoss()\n",
        "            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n",
        "            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999), amsgrad=opt.amsgrad)\n",
        "            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999), amsgrad=opt.amsgrad)\n",
        "            self.optimizers.append(self.optimizer_G)\n",
        "            self.optimizers.append(self.optimizer_D)\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n",
        "        self.fake_B = self.netG(self.real_A)  # G(A)\n",
        "\n",
        "    def backward_D(self):\n",
        "        \"\"\"Calculate GAN loss for the discriminator\"\"\"\n",
        "        # Fake; stop backprop to the generator by detaching fake_B\n",
        "        fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator\n",
        "        pred_fake = self.netD(fake_AB.detach())\n",
        "        self.loss_D_fake = self.criterionGAN(pred_fake, False)\n",
        "        # Real\n",
        "        real_AB = torch.cat((self.real_A, self.real_B), 1)\n",
        "        pred_real = self.netD(real_AB)\n",
        "        self.loss_D_real = self.criterionGAN(pred_real, True)\n",
        "        # combine loss and calculate gradients\n",
        "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
        "        self.loss_D.backward()\n",
        "        self.pred_fake = pred_fake\n",
        "        self.pred_real = pred_real\n",
        "\n",
        "    def backward_G(self):\n",
        "        \"\"\"Calculate GAN and L1 loss for the generator\"\"\"\n",
        "        # First, G(A) should fake the discriminator\n",
        "        fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n",
        "        pred_fake = self.netD(fake_AB)\n",
        "        self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n",
        "        # Second, G(A) = B\n",
        "        self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1\n",
        "        self.loss_G_MSE = self.criterionMSE(self.fake_B, self.real_B) * self.opt.lambda_MSE\n",
        "        # combine loss and calculate gradients\n",
        "        self.loss_G = self.loss_G_GAN + self.loss_G_L1 + self.loss_G_MSE\n",
        "        self.loss_G.backward()\n",
        "\n",
        "    def optimize_parameters(self):\n",
        "        self.forward()                   # compute fake images: G(A)\n",
        "        # update D\n",
        "        self.set_requires_grad(self.netD, True)  # enable backprop for D\n",
        "        self.optimizer_D.zero_grad()     # set D's gradients to zero\n",
        "        self.backward_D()                # calculate gradients for D\n",
        "        self.optimizer_D.step()          # update D's weights\n",
        "        # update G\n",
        "        self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G\n",
        "        self.optimizer_G.zero_grad()        # set G's gradients to zero\n",
        "        self.backward_G()                   # calculate graidents for G\n",
        "        self.optimizer_G.step()             # udpate G's weights\n",
        "\n",
        "    def setup(self, opt):\n",
        "        \"\"\"Load and print networks; create schedulers\n",
        "\n",
        "        Parameters:\n",
        "            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
        "        \"\"\"\n",
        "        if self.isTrain:\n",
        "            self.schedulers = [get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n",
        "        self.print_networks(opt.verbose)\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Make models eval mode during test time\"\"\"\n",
        "        for name in self.model_names:\n",
        "            if isinstance(name, str):\n",
        "                net = getattr(self, 'net' + name)\n",
        "                net.eval()\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"Forward function used in test time.\n",
        "\n",
        "        This function wraps <forward> function in no_grad() so we don't save intermediate steps for backprop\n",
        "        It also calls <compute_visuals> to produce additional visualization results\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            self.forward()\n",
        "            self.compute_visuals()\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        \"\"\"Update learning rates for all the networks; called at the end of every epoch\"\"\"\n",
        "        old_lr = self.optimizers[0].param_groups[0]['lr']\n",
        "        for scheduler in self.schedulers:\n",
        "            scheduler.step()\n",
        "\n",
        "        lr = self.optimizers[0].param_groups[0]['lr']\n",
        "        print('learning rate %.7f -> %.7f' % (old_lr, lr))\n",
        "\n",
        "    def save_networks(self, epoch):\n",
        "        \"\"\"Save all the networks to the disk.\n",
        "\n",
        "        Parameters:\n",
        "            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n",
        "        \"\"\"\n",
        "        for name in self.model_names:\n",
        "            if isinstance(name, str):\n",
        "                save_filename = '%s_net_%s.pth' % (epoch, name)\n",
        "                save_path = os.path.join(self.save_dir, save_filename)\n",
        "                net = getattr(self, 'net' + name)\n",
        "                if isinstance(net, torch.nn.DataParallel):\n",
        "                    net = net.module\n",
        "                    \n",
        "                torch.save(net.state_dict(), save_path)\n",
        "\n",
        "    def load_networks(self, epoch):\n",
        "        \"\"\"Load all the networks from the disk.\n",
        "\n",
        "        Parameters:\n",
        "            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n",
        "        \"\"\"\n",
        "        for name in self.model_names:\n",
        "            if isinstance(name, str):\n",
        "                load_filename = '%s_net_%s.pth' % (epoch, name)\n",
        "                load_path = os.path.join(self.save_dir, load_filename)\n",
        "                net = getattr(self, 'net' + name)\n",
        "                if isinstance(net, torch.nn.DataParallel):\n",
        "                    net = net.module\n",
        "                print('loading the model from %s' % load_path)\n",
        "                # if you are using PyTorch newer than 0.4 (e.g., built from\n",
        "                # GitHub source), you can remove str() on self.device\n",
        "                state_dict = torch.load(load_path, map_location=self.devices[0])\n",
        "                if hasattr(state_dict, '_metadata'):\n",
        "                    del state_dict._metadata\n",
        "\n",
        "                net.load_state_dict(state_dict)\n",
        "\n",
        "    def print_networks(self, verbose):\n",
        "        \"\"\"Print the total number of parameters in the network and (if verbose) network architecture\n",
        "\n",
        "        Parameters:\n",
        "            verbose (bool) -- if verbose: print the network architecture\n",
        "        \"\"\"\n",
        "        print('---------- Networks initialized -------------')\n",
        "        for name in self.model_names:\n",
        "            if isinstance(name, str):\n",
        "                net = getattr(self, 'net' + name)\n",
        "                num_params = 0\n",
        "                for param in net.parameters():\n",
        "                    num_params += param.numel()\n",
        "                    if param.dtype != self.dtype:\n",
        "                      print(f\"Warning! Parameter has wrong dtype {param.dtype}, {param.device}, {param.numel()}. Expected {self.dtype}\")\n",
        "                    if str(param.device) != str(self.opt.devices[0]):\n",
        "                      print(f\"Warning! Parameter has wrong device {param.dtype}, {param.device}, {param.numel()}. Expected {self.opt.devices[0]}\")\n",
        "                for buffer in net.buffers():\n",
        "                    if buffer.dtype != self.dtype:\n",
        "                      print(f\"Warning! Buffer has wrong dtype {buffer.dtype}, {buffer.device}, {buffer.numel()}. Expected {self.dtype}\")\n",
        "                    if str(buffer.device) != str(self.opt.devices[0]):\n",
        "                      print(f\"Warning! Buffer has wrong device {buffer.dtype}, {buffer.device}, {buffer.numel()}. Expected {self.opt.devices[0]}\")\n",
        "                if verbose:\n",
        "                    print(net)\n",
        "                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n",
        "        print('-----------------------------------------------')\n",
        "\n",
        "    def set_requires_grad(self, nets, requires_grad=False):\n",
        "        \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
        "        Parameters:\n",
        "            nets (network list)   -- a list of networks\n",
        "            requires_grad (bool)  -- whether the networks require gradients or not\n",
        "        \"\"\"\n",
        "        if not isinstance(nets, list):\n",
        "            nets = [nets]\n",
        "        for net in nets:\n",
        "            if net is not None:\n",
        "                for param in net.parameters():\n",
        "                    param.requires_grad = requires_grad\n",
        "\n",
        "    def to(self, *args):\n",
        "      self.netG.to(*args)\n",
        "      self.netD.to(*args)\n"
      ],
      "metadata": {
        "id": "iyjkOXSqXxld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model configuration"
      ],
      "metadata": {
        "id": "kQotN5mtzBUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainOptions():\n",
        "    pass\n",
        "\n",
        "opt = TrainOptions()\n",
        "opt.checkpoints_dir = '/content/drive/MyDrive/AI/pix2pix/checkpoints'\n",
        "opt.name = \"denoising9-medium\"  #@param {type: \"string\"}\n",
        "\n",
        "#@markdown ## Model parameters\n",
        "#@markdown **Changing these will invalidate checkpoint files!**\n",
        "opt.input_nc = opt.output_nc = 3\n",
        "opt.ngf = 64              #@param {type: \"integer\"}\n",
        "opt.ndf = 64              #@param {type: \"integer\"}\n",
        "opt.attention_dim = 32    #@param {type: \"integer\"}\n",
        "\n",
        "#@markdown ## Training parameters\n",
        "opt.init_type = \"normal\"\n",
        "opt.init_gain = 0.02\n",
        "opt.gan_mode = \"vanilla\"    #@param [\"vanilla\", \"lsgan\"]\n",
        "opt.beta1 = 0.9           #@param {type: \"number\"}\n",
        "opt.lr = 0.0002           #@param {type: \"number\"}\n",
        "opt.amsgrad = True        #@param {type: \"boolean\"}\n",
        "opt.epoch_count = 1\n",
        "opt.n_epochs = 100\n",
        "opt.n_epochs_decay = 100\n",
        "opt.batch_size = 4        #@param {type: \"integer\"}\n",
        "opt.noise_schedule_beta = 0.003   #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ## Objective parameters\n",
        "opt.lambda_L1 = 5       #@param {type: \"number\"}\n",
        "opt.lambda_MSE = 50    #@param {type: \"number\"}\n",
        "\n",
        "#@markdown ## Debugging\n",
        "opt.verbose = False       #@param {type: 'boolean'}\n",
        "opt.display_interval = 10 #@param {type: \"integer\"}\n",
        "opt.debug_xla = False      #@param {type: 'boolean'}\n",
        "\n",
        "#@markdown ## Backend selection\n",
        "opt.backend = 'CUDA' #@param [\"CPU\", \"CUDA\", \"TPU\"]\n",
        "opt.dtype = 'float32' #@param [\"float32\", \"bfloat16\"]\n",
        "\n",
        "if opt.backend == 'CUDA':\n",
        "  torch.cuda.set_device(0)\n",
        "  opt.devices = [torch.device('cuda:0')]\n",
        "elif opt.backend == 'TPU':\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  opt.devices = [xm.xla_device()]\n",
        "else:\n",
        "  opt.devices = [torch.device('cpu')]\n",
        "\n",
        "if opt.dtype == \"bfloat16\":\n",
        "  opt.dtype = torch.bfloat16\n",
        "else:\n",
        "  opt.dtype = torch.float\n",
        "\n"
      ],
      "metadata": {
        "id": "o4iRw1MpzDvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "AlfZaNA3XqY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, random\n",
        "import torch\n",
        "import torchvision as tv\n",
        "import einops\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "from torchvision.datasets import ImageFolder\n",
        "from IPython import display\n",
        "\n",
        "dataset = tv.datasets.ImageFolder(root = DATA_DIR, \n",
        "  transform = tv.transforms.Compose([\n",
        "    #tv.transforms.Resize(256),                                                            \n",
        "    tv.transforms.ToTensor(),\n",
        "  ]))\n",
        "if opt.backend == 'TPU':\n",
        "  sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "  data_loader = torch.utils.data.DataLoader(dataset,\n",
        "    sampler=sampler,\n",
        "    batch_size=opt.batch_size,\n",
        "    num_workers=4,\n",
        "    drop_last=True)\n",
        "else:\n",
        "  data_loader = torch.utils.data.DataLoader(dataset,\n",
        "    batch_size=opt.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4)\n",
        "\n",
        "\n",
        "opt.isTrain = True\n",
        "opt.continue_train = False\n",
        "\n",
        "RUN_DIR = f\"{opt.checkpoints_dir}/{opt.name}\"\n",
        "!mkdir -p $RUN_DIR\n",
        "\n",
        "model = Pix2PixModel(opt)\n",
        "model.setup(opt)\n",
        "\n",
        "start_epoch = 1\n",
        "last_epoch = 100\n",
        "# Resume\n",
        "try:\n",
        "  cp = open(f\"{opt.checkpoints_dir}/{opt.name}/latest_epoch\", \"r\")\n",
        "  start_epoch = int(cp.read())\n",
        "  model.load_networks(str(start_epoch))\n",
        "  start_epoch += 1\n",
        "  cp.close()\n",
        "except:\n",
        "  print(f\"Failed to load checkpoint {start_epoch}\")\n",
        "  start_epoch = 1\n",
        "  pass\n",
        "\n",
        "arch = open(f\"{opt.checkpoints_dir}/{opt.name}/architecture\", \"w\")\n",
        "arch.write(f\"netG: {model.netG}\\nnetD: {model.netD}\\n\")\n",
        "arch.close()\n",
        "\n",
        "#display.clear_output(wait=True)\n",
        "#dataset[1000][0]\n",
        "\n",
        "crop = tv.transforms.RandomCrop(256)\n",
        "\n",
        "for epoch in range(start_epoch, last_epoch+1):\n",
        "  model.update_learning_rate()\n",
        "  for i, data in enumerate(data_loader):\n",
        "    output_data = crop(data[0].to(opt.dtype).to(opt.devices[0])) * 2 - 1\n",
        "    time = torch.rand(size = (output_data.size()[0], ), device=opt.devices[0], dtype=opt.dtype) * 1000\n",
        "    beta = einops.rearrange((1 - opt.noise_schedule_beta) ** time, 'd -> d 1 1 1')\n",
        "    input_data = (output_data * torch.sqrt(1 - beta) + torch.randn_like(output_data) * torch.sqrt(beta))\n",
        "\n",
        "    model.real_A = input_data.to(opt.dtype)\n",
        "    model.real_B = output_data.to(opt.dtype)\n",
        "\n",
        "    model.optimize_parameters()\n",
        "\n",
        "    if opt.backend == 'TPU' and opt.debug_xla:\n",
        "      print(torch_xla._XLAC._get_xla_tensors_text([model.loss_G_GAN, model.loss_G_L1, model.loss_G_MSE, model.loss_D_fake, model.loss_D_real]))\n",
        "\n",
        "    if opt.display_interval > 0 and i % opt.display_interval == 0:\n",
        "      display.clear_output(wait=True)\n",
        "      model.print_networks(verbose=opt.verbose)\n",
        "      print(f\"epoch {epoch}/{last_epoch} batch {i}/{len(data_loader)}\")\n",
        "      if opt.backend != 'TPU':\n",
        "        sample_index = 0\n",
        "        pred_fake = nn.functional.adaptive_avg_pool3d(model.pred_fake[sample_index], 1)\n",
        "        pred_real = nn.functional.adaptive_avg_pool3d(model.pred_real[sample_index], 1)\n",
        "        sample = to_pil_image(torch.cat((model.real_A[sample_index],\n",
        "          model.real_B[sample_index],\n",
        "          model.fake_B[sample_index]), dim=2).clamp(-1, 1) * 0.5 + 0.5)\n",
        "        print(f\"Sample {sample_index}: time {time[sample_index]}, beta {beta[sample_index][0][0][0]}, pred_fake {pred_fake[0][0][0]}, pred_real {pred_real[0][0][0]}\")\n",
        "        display.display_png(sample)\n",
        "      print(f\"Batch losses: G_GAN {model.loss_G_GAN}, G_L1 {model.loss_G_L1}, G_MSE {model.loss_G_MSE}, D_fake {model.loss_D_fake}, D_real {model.loss_D_real}\")\n",
        "  \n",
        "  #del model.real_A\n",
        "  #del model.real_B\n",
        "  #del model.fake_B\n",
        "  model.save_networks('latest')\n",
        "  model.save_networks(epoch)\n",
        "  cp = open(f\"{opt.checkpoints_dir}/{opt.name}/latest_epoch\", \"w\")\n",
        "  cp.write(str(epoch))\n",
        "  \n",
        "  cp.close()\n"
      ],
      "metadata": {
        "id": "3dNW0jxvb60P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denoise"
      ],
      "metadata": {
        "id": "cPM77ZOm4uST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, io, requests\n",
        "import torch\n",
        "import einops\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor, to_pil_image, resize\n",
        "from IPython import display\n",
        "\n",
        "image_url = \"https://i.imgur.com/3NHuxu1.png\" #@param {type: \"string\"}\n",
        "image_size = [512,512] #@param {type: \"raw\"}\n",
        "noise_beta =  0#@param {type: \"number\"}\n",
        "iterations =  1#@param {type: \"integer\", min:1}\n",
        "\n",
        "#@markdown ## NN visualization\n",
        "visualize_layer = \"\" #@param {type: \"string\"}\n",
        "visualize_channels = [0, 64] #@param {type: \"raw\"}\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "opt.isTrain = False\n",
        "\n",
        "model = Pix2PixModel(opt)\n",
        "model.setup(opt)\n",
        "model.load_networks('latest')\n",
        "\n",
        "if image_url != \"\":\n",
        "  pil_image = Image.open(fetch(image_url)).convert('RGB')\n",
        "\n",
        "  if image_size != None:\n",
        "    pil_image = resize(pil_image, image_size)\n",
        "\n",
        "  print(\"Input image\")\n",
        "  display.display_png(pil_image)\n",
        "\n",
        "  image = to_tensor(pil_image).to(model.devices[0])\n",
        "  image = einops.rearrange(image, '... -> 1 ...')\n",
        "  image = image * 2 - 1\n",
        "else:\n",
        "  if image_size == None:\n",
        "    image_size = [256, 256]\n",
        "  image = torch.zeros(1, 3, image_size[0], image_size[1]).to(model.devices[0])\n",
        "\n",
        "image = image * math.sqrt(1 - noise_beta) + torch.randn_like(image) * math.sqrt(noise_beta)\n",
        "\n",
        "model.set_requires_grad(model.netG, False)\n",
        "\n",
        "cum_beta = noise_beta\n",
        "for step in range(0, iterations):\n",
        "  step_beta = 1 / (iterations - step)\n",
        "\n",
        "  iter = model.netG(image)\n",
        "  image = image * (1 - step_beta) + iter * (step_beta)\n",
        "  cum_beta *= (1 - step_beta)\n",
        "\n",
        "  if step + 1 < iterations:\n",
        "    print(f\"Iteration {step+1}/{iterations}\")\n",
        "    display.display_png(to_pil_image(iter[0].clamp(-1, 1) * 0.5 + 0.5))\n",
        "\n",
        "print(\"Final result\")\n",
        "display.display_png(to_pil_image(image[0].clamp(-1, 1) * 0.5 + 0.5))\n"
      ],
      "metadata": {
        "id": "6fVpqu5u4vlk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}